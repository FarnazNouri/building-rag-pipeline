{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b805c3cd",
   "metadata": {},
   "source": [
    "# LangChain Document Structure\n",
    "\n",
    "## from langchain.schema import Document\n",
    "\n",
    "### Core Components:\n",
    "    - page_content(str)\n",
    "    - metadata(dict)\n",
    "\n",
    "### LangChain Loader\n",
    "    - give you the content of a data (csv, pdf,...) to a document structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cdf47b",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca5f1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Structure\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86f7defe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'Farnaz Nouri', 'data_created': '2025-10-01'}, page_content='this is the main text content I am using to create RAG')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document(\n",
    "    page_content = \"this is the main text content I am using to create RAG\",\n",
    "    metadata = {\n",
    "        'source': 'example.txt',\n",
    "        'pages': 1,\n",
    "        'author': \"Farnaz Nouri\",\n",
    "        'data_created': '2025-10-01'\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e2acc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple txt file\n",
    "import os\n",
    "os.makedirs('../data/text_files', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8e0d822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample text file created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts = {\n",
    "    \"../data/text_files/python_intro.txt\":'''Python is a high-level, interpreted programming language known for its readability and versatility. Created by Guido van Rossum and released in 1991, it has become one of the most popular languages for various applications.\n",
    "Key Characteristics:\n",
    "Readability: Python's syntax emphasizes clarity and conciseness, often allowing developers to express concepts in fewer lines of code compared to other languages. This is partly due to its use of indentation to define code blocks.\n",
    "Interpreted: Python code is executed line by line by an interpreter, which facilitates rapid prototyping and interactive testing.\n",
    "Dynamically Typed: Variable types are automatically determined at runtime, simplifying code writing as explicit type declarations are not always required.\n",
    "Multi-paradigm: Python supports various programming paradigms, including object-oriented, procedural, and functional programming.\n",
    "Extensive Standard Library: Python comes with a large standard library that provides modules and functions for a wide range of tasks, reducing the need to write code from scratch.\n",
    "Cross-platform: Python applications can be developed and run on different operating systems like Windows, macOS, and Linux.\n",
    "Common Applications:\n",
    "Web Development: Used for server-side web applications with frameworks like Django and Flask.\n",
    "Data Science and Machine Learning: A popular choice due to its powerful libraries such as NumPy, Pandas, and scikit-learn.\n",
    "Automation and Scripting: Ideal for automating repetitive tasks and system administration.\n",
    "Software Development: Used for creating desktop applications, games, and internal tools.\n",
    "Scientific Computing and Research: Employed in various scientific fields for data analysis and modeling.\n",
    "    ''',\n",
    "    \"../data/text_files/machine_learning.txt\": ''' \n",
    "Machine learning allows computers to learn from data and improve performance on tasks without explicit programming. The core process involves collecting and preparing data, selecting an algorithm, training a model, and evaluating its accuracy to make predictions. The three primary types of machine learning are supervised learning (using labeled data for tasks like classification and regression), unsupervised learning (finding patterns in unlabeled data, like clustering), and reinforcement learning (learning from rewards and penalties in an environment).\n",
    "Key Concepts\n",
    "Data is Crucial: High-quality, diverse data is the foundation of machine learning, providing the examples for models to learn from. \n",
    "Algorithms & Models: An algorithm is a set of instructions that enables the computer to learn from data, while a model is the trained output of the algorithm. \n",
    "Features: These are the attributes or characteristics extracted from data that are used by the model to learn and make decisions. \n",
    "Types of Machine Learning\n",
    "Supervised Learning:\n",
    "How it works: Uses labeled datasets, where the correct input-output relationships are known, to train the model. \n",
    "Examples: Image recognition (labeling photos as \"cat\" or \"dog\") and spam email filtering. \n",
    "Unsupervised Learning:\n",
    "How it works: Works with unlabeled data to identify hidden patterns, similarities, and groupings on its own. \n",
    "Examples: Clustering data points to find common characteristics or detecting anomalies in datasets. \n",
    "Reinforcement Learning:\n",
    "How it works: An agent learns by interacting with an environment, receiving feedback in the form of rewards or penalties for its actions. \n",
    "Examples: Training a robot to navigate or a program to play a game by playing against itself.\n",
    "'''\n",
    "}\n",
    "\n",
    "for filepath, content in sample_texts.items():\n",
    "    with open(filepath, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"sample text file created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef930a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content=\"Python is a high-level, interpreted programming language known for its readability and versatility. Created by Guido van Rossum and released in 1991, it has become one of the most popular languages for various applications.\\nKey Characteristics:\\nReadability: Python's syntax emphasizes clarity and conciseness, often allowing developers to express concepts in fewer lines of code compared to other languages. This is partly due to its use of indentation to define code blocks.\\nInterpreted: Python code is executed line by line by an interpreter, which facilitates rapid prototyping and interactive testing.\\nDynamically Typed: Variable types are automatically determined at runtime, simplifying code writing as explicit type declarations are not always required.\\nMulti-paradigm: Python supports various programming paradigms, including object-oriented, procedural, and functional programming.\\nExtensive Standard Library: Python comes with a large standard library that provides modules and functions for a wide range of tasks, reducing the need to write code from scratch.\\nCross-platform: Python applications can be developed and run on different operating systems like Windows, macOS, and Linux.\\nCommon Applications:\\nWeb Development: Used for server-side web applications with frameworks like Django and Flask.\\nData Science and Machine Learning: A popular choice due to its powerful libraries such as NumPy, Pandas, and scikit-learn.\\nAutomation and Scripting: Ideal for automating repetitive tasks and system administration.\\nSoftware Development: Used for creating desktop applications, games, and internal tools.\\nScientific Computing and Research: Employed in various scientific fields for data analysis and modeling.\\n    \")]\n"
     ]
    }
   ],
   "source": [
    "# TextLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/text_files/python_intro.txt\", encoding='utf-8')\n",
    "document = loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c5d32e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content=\"Python is a high-level, interpreted programming language known for its readability and versatility. Created by Guido van Rossum and released in 1991, it has become one of the most popular languages for various applications.\\nKey Characteristics:\\nReadability: Python's syntax emphasizes clarity and conciseness, often allowing developers to express concepts in fewer lines of code compared to other languages. This is partly due to its use of indentation to define code blocks.\\nInterpreted: Python code is executed line by line by an interpreter, which facilitates rapid prototyping and interactive testing.\\nDynamically Typed: Variable types are automatically determined at runtime, simplifying code writing as explicit type declarations are not always required.\\nMulti-paradigm: Python supports various programming paradigms, including object-oriented, procedural, and functional programming.\\nExtensive Standard Library: Python comes with a large standard library that provides modules and functions for a wide range of tasks, reducing the need to write code from scratch.\\nCross-platform: Python applications can be developed and run on different operating systems like Windows, macOS, and Linux.\\nCommon Applications:\\nWeb Development: Used for server-side web applications with frameworks like Django and Flask.\\nData Science and Machine Learning: A popular choice due to its powerful libraries such as NumPy, Pandas, and scikit-learn.\\nAutomation and Scripting: Ideal for automating repetitive tasks and system administration.\\nSoftware Development: Used for creating desktop applications, games, and internal tools.\\nScientific Computing and Research: Employed in various scientific fields for data analysis and modeling.\\n    \"),\n",
       " Document(metadata={'source': '../data/text_files/machine_learning.txt'}, page_content=' \\nMachine learning allows computers to learn from data and improve performance on tasks without explicit programming. The core process involves collecting and preparing data, selecting an algorithm, training a model, and evaluating its accuracy to make predictions. The three primary types of machine learning are supervised learning (using labeled data for tasks like classification and regression), unsupervised learning (finding patterns in unlabeled data, like clustering), and reinforcement learning (learning from rewards and penalties in an environment).\\nKey Concepts\\nData is Crucial: High-quality, diverse data is the foundation of machine learning, providing the examples for models to learn from. \\nAlgorithms & Models: An algorithm is a set of instructions that enables the computer to learn from data, while a model is the trained output of the algorithm. \\nFeatures: These are the attributes or characteristics extracted from data that are used by the model to learn and make decisions. \\nTypes of Machine Learning\\nSupervised Learning:\\nHow it works: Uses labeled datasets, where the correct input-output relationships are known, to train the model. \\nExamples: Image recognition (labeling photos as \"cat\" or \"dog\") and spam email filtering. \\nUnsupervised Learning:\\nHow it works: Works with unlabeled data to identify hidden patterns, similarities, and groupings on its own. \\nExamples: Clustering data points to find common characteristics or detecting anomalies in datasets. \\nReinforcement Learning:\\nHow it works: An agent learns by interacting with an environment, receiving feedback in the form of rewards or penalties for its actions. \\nExamples: Training a robot to navigate or a program to play a game by playing against itself.\\n')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directory Loader -> if you have all the documents in your directory \n",
    "# and want to load all of them\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# Load all the text files from the directory\n",
    "dir_loader = DirectoryLoader(\n",
    "    '../data/text_files',\n",
    "    glob= \"**/*.txt\", # pattern to match files - corrected pattern\n",
    "    loader_cls= TextLoader,\n",
    "    loader_kwargs={'encoding':'utf-8'},\n",
    "    show_progress=False\n",
    ")\n",
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "403a88be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'attention', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='NLP,  attention  mechanisms  enable  a  model  to  dynamically  weigh  the  importance  of  different  \\nwords\\n \\nin\\n \\nan\\n \\ninput\\n \\nsequence,\\n \\nallowing\\n \\nit\\n \\nto\\n \\nfocus\\n \\non\\n \\nthe\\n \\nmost\\n \\nrelevant\\n \\nparts\\n \\nof\\n \\nthe\\n \\ncontext\\n \\nwhen\\n \\nprocessing\\n \\ninformation\\n \\nor\\n \\ngenerating\\n \\noutput.\\n \\nThis\\n \\nimproves\\n \\nunderstanding\\n \\nof\\n \\nlong-range\\n \\ndependencies,\\n \\nlike\\n \\n\"dog\"\\n \\nand\\n \\n\"field\"\\n \\nin\\n \\n\"The\\n \\ndog\\n \\nran\\n \\nacross\\n \\nthe\\n \\nfield,\"\\n \\nand\\n \\nis\\n \\na\\n \\nfoundational\\n component  of  modern  Transformer  models,  powering  tasks  from  text  generation to  translation.    How  Attention  Works  At  its  core,  attention  works  by  using  the  concepts  of  queries,  keys,  and  values  to  determine  how  \\nrelevant\\n \\ndifferent\\n \\nparts\\n \\nof\\n \\nthe\\n \\ninput\\n \\nsequence\\n \\nare\\n \\nto\\n \\na\\n \\ngiven\\n \\nword\\n \\nor\\n \\noutput.\\n \\n ●  Query  (Q):  Represents  the  current  input  (e.g.,  the  word  being  processed)  that  is  looking  \\nfor\\n \\nrelevant\\n \\ninformation.\\n \\n ●   ●  Keys  (K):  Act  as  \"labels\"  or  identifiers  for  each  part  of  the  input  data.   ●   ●  Values  (V):  Contain  the  actual  information  that  will  be  retrieved  or  weighted.   ●   The  model  calculates  an  \"attention  score\"  by  comparing  the  query  with  each  key.These  scores  are  then  normalized  using  a  softmax  function to  produce  attention  weights.  Finally,  these  weights  are  used  to  compute  a  weighted  sum  of  the  corresponding  values,  producing  a  refined  \\nrepresentation\\n \\nthat\\n \\nemphasizes\\n \\nthe\\n \\nmost\\n \\nimportant\\n \\ncontext.\\n \\n Key  Types  of  Attention  ●  Self-Attention:  A  crucial  innovation  in  the  Transformer  architecture,  where  queries,  keys,  and  values  are  all  derived  from  the  same  input  sequence.  This  allows  tokens  within  the  \\nsequence\\n \\nto\\n \\nattend\\n \\nto\\n \\neach\\n \\nother,\\n \\ncapturing\\n \\ninternal\\n \\nrelationships\\n \\nand\\n \\ncontext.\\n \\n ●  Multi-Head  Attention:  In  this  technique,  multiple  attention  mechanisms  are  run  in  parallel.  Each  \"head\"  can  learn  different  types  of  relationships,  allowing  the  model  to  grasp  \\nvarious\\n \\npatterns\\n \\nand\\n \\nnuances\\n \\nwithin\\n \\nthe\\n \\ntext.\\n \\n Why  Attention  is  Important  ●  Improved  Contextual  Understanding:  Attention  allows  models  to  capture  dependencies  \\nbetween\\n \\nwords\\n \\nthat\\n \\nare\\n \\nfar\\n \\napart\\n \\nin\\n \\na\\n \\nsentence,\\n \\nleading\\n \\nto\\n \\na\\n \\nricher\\n \\nunderstanding\\n \\nof\\n \\nmeaning.\\n \\n ●  Enhanced  Performance:  It  has  significantly  improved  performance  in  many  NLP  tasks,  \\nincluding\\n \\ntext\\n \\ngeneration,\\n \\ntranslation,\\n \\nand\\n \\ntext\\n \\nsummarization.\\n \\n ●  Computational  Efficiency:  By  enabling  parallel  processing  of  sequences,  attention  mechanisms  are  more  computationally  efficient  than  older,  recurrent  neural  network  (RNN) based  models,  which  process  information  sequentially.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-08-07T19:40:52+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-08-07T19:40:52+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '../data/pdf_files/Technical_Topics_Cheat_Sheet.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content=\"End-to-End ML Pipeline\\nImagine you’re baking a cake. You gather ingredients (data ingestion), mix and prepare them (data\\ncleaning, feature engineering), bake the cake (train your model), taste-test it (evaluate), then serve it to\\nothers (deploy and monitor). That’s an ML pipeline — taking data from raw to useful predictions.\\nSupervised, Unsupervised & Reinforcement Learning\\nSupervised: You give the computer the question and the answer, and it learns to match them (like\\nteaching with flashcards). Unsupervised: You just give the computer the data, and it finds patterns on\\nits own (like sorting LEGO pieces by shape without labels). Reinforcement: The computer learns by\\ntrying, getting rewards for good actions (like a dog learning tricks with treats).\\nTime Series Forecasting & Monte Carlo Simulation\\nTime series is like trying to guess tomorrow’s weather based on past patterns. Monte Carlo simulation\\nis like rolling dice many times to guess the chances of something happening — helpful when you’re\\nunsure and want to understand all possible outcomes.\\nNLP & LLMs\\nNLP is how computers read, understand, and write human language. LLMs (like ChatGPT) are trained\\non lots of text and can answer questions, write summaries, or even draft emails — like a very smart\\nintern who reads a million books a day.\\nMLOps + Cloud Deployment\\nMLOps is like DevOps for ML — it helps you build, ship, and maintain ML models smoothly. Cloud tools\\nlike AWS let you run projects on someone else’s supercomputer, so you can scale models reliably, like\\nturning a food truck into a restaurant chain.\\nData Visualization & Storytelling\\nData visualization turns numbers into pictures so people can see patterns — think graphs, charts,\\ndashboards. Storytelling is explaining those patterns so someone can make a smart decision — like\\nturning a traffic map into 'Take Route B — it’s faster!'\\nReal-World Data (RWD)\\nRWD is messy, like kids’ drawings on paper — lots of creativity, but not always clean. It comes from\\nreal people: hospital records, prescriptions, sales logs. You have to clean it, match things up, and make\\nsure it’s trustworthy before using it.\\nVersion Control & Collaboration (Git, Agile)\\nGit is like Google Docs for code — it tracks changes, lets you go back, and lets your team work\\ntogether. Agile means breaking big jobs into small pieces, sharing updates often, and adjusting as you\\ngo — like building LEGO castles one section at a time.\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'embedding', 'source': '../data/pdf_files/embedding.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='●   An  embedding  is  a  data  representation  that  uses  low-dimensional  numerical  vectors  to  capture  \\nthe\\n \\nsemantic\\n \\nmeaning\\n \\nand\\n \\nrelationships\\n \\nof\\n \\nnon-numerical\\n \\ndata\\n \\nlike\\n \\nwords,\\n \\nimages,\\n \\nor\\n \\ngraphs,\\n \\nmaking\\n \\nthem\\n \\nunderstandable\\n \\nfor\\n \\nmachine\\n \\nlearning\\n \\nmodels.\\n \\nThese\\n \\ndense\\n \\nvectors\\n \\nare\\n \\ncreated\\n \\nthrough\\n \\na\\n \\nprocess\\n \\nof\\n \\nmapping\\n \\ncomplex,\\n \\nreal-world\\n \\ndata\\n \\ninto\\n \\na\\n \\nvector\\n \\nspace,\\n \\nwhere\\n \\nthe\\n \\ndistance\\n \\nbetween\\n \\nembeddings\\n \\nreflects\\n \\nthe\\n \\nsimilarity\\n \\nof\\n \\nthe\\n \\noriginal\\n \\ndata\\n \\npoints.\\n \\nFor\\n \\ninstance,\\n \\nin\\n \\nnatural\\n \\nlanguage\\n \\nprocessing\\n \\n(NLP),\\n \\nword\\n \\nembeddings\\n \\nallow\\n \\nfor\\n \\nmathematical\\n \\noperations\\n \\nthat\\n \\nrepresent\\n \\nrelationships,\\n \\nlike\\n \\nking\\n \\n-\\n \\nman\\n \\n+\\n \\nwoman\\n \\n≈\\n \\nqueen,\\n \\nto\\n \\nuncover\\n \\ndeeper\\n \\nmeaning\\n \\nand\\n \\nenable\\n \\ntasks\\n \\nsuch\\n \\nas\\n \\nsentiment\\n \\nanalysis\\n \\nand\\n \\nmachine\\n \\ntranslation.\\n  \\n What  is  the  process?  ●  Data  Transformation:  Non-numerical  data  (text,  images,  graphs)  is  converted  into  numerical  vectors,  where  each  dimension  represents  a  specific  feature  of  the  data.   ●  Vector  Space:  These  vectors  are  placed  into  an  n-dimensional  space,  creating  a  dense  numerical  representation.   ●  Semantic  Relationships:  The  embedding  process  captures  the  nuances  and  context  of  the  original  data.  For  example,  words  with  similar  meanings  will  have  embeddings  that  \\nare\\n \\ncloser\\n \\ntogether\\n \\nin\\n \\nthe\\n \\nvector\\n \\nspace.\\n \\n ●  Machine  Learning  Use:  These  vectors  can  then  be  used  by  machine  learning  models  for  tasks  like  classification,  search,  recommendation  systems,  and  more.   Key  Characteristics  and  Benefits  ●  Information  Density:  Embeddings  provide  an  information-dense  representation,  meaning  \\nthey\\n \\npack\\n \\na\\n \\nlot\\n \\nof\\n \\nmeaning\\n \\ninto\\n \\nfewer\\n \\ndimensions\\n \\nthan\\n \\ntraditional\\n \\nmethods.\\n \\n ●  Semantic  Similarity:  The  distance  between  two  embeddings  in  the  vector  space  \\ncorrelates\\n \\nwith\\n \\nthe\\n \\nsemantic\\n \\nsimilarity\\n \\nof\\n \\nthe\\n \\noriginal\\n \\ninputs.\\n \\n ●  Machine-Readable  Format:  Embeddings  make  complex  data  understandable  to  \\ncomputers\\n \\nand\\n \\nalgorithms,\\n \\nwhich\\n \\nonly\\n \\nprocess\\n \\nnumerical\\n \\ninputs.\\n \\n ●  Capture  of  Relationships:  Embeddings  can  capture  intricate  relationships  within  the  data,  \\nsuch\\n \\nas\\n \\nco-occurrence\\n \\npatterns\\n \\nin\\n \\ntext\\n \\nor\\n \\nstructural\\n \\nrelationships\\n \\nin\\n \\ngraphs.\\n \\n Examples  of  Embeddings  ●  Word  Embeddings:  Represent  words  as  vectors,  enabling  tasks  like  sentiment  analysis  and  machine  translation.   ●  Image  Embeddings:  Map  images  into  a  vector  space  to  group  similar  images  and  enable  visual  content  retrieval.   ●  Graph  Embeddings:  Convert  nodes  and  edges  in  graphs  into  numerical  vectors  to  understand  relationships  and  predict  links.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directory Loader for PDF files\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "\n",
    "# First, let's create the pdf_files directory and add a sample PDF (if needed)\n",
    "import os\n",
    "os.makedirs('../data/pdf_files', exist_ok=True)\n",
    "\n",
    "# Load all the PDF files from the directory\n",
    "dir_loader = DirectoryLoader(\n",
    "    '../data/pdf_files',\n",
    "    glob= \"**/*.pdf\", # pattern to match files - corrected pattern\n",
    "    loader_cls= PyPDFLoader,\n",
    "    show_progress=False\n",
    ")\n",
    "pdf_documents = dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ecd4c7",
   "metadata": {},
   "source": [
    "# Embedding And VectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4a3c7d",
   "metadata": {},
   "source": [
    "# Text Chunking\n",
    "\n",
    "Before creating embeddings, we need to split our documents into smaller chunks for better retrieval performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "610da8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining text and PDF documents...\n",
      "Text documents: 2\n",
      "PDF documents: 3\n",
      "Total documents: 5\n",
      "\n",
      "Splitting documents into chunks...\n",
      "Number of original documents: 5\n",
      "Number of chunks after splitting: 15\n",
      "\n",
      "Example chunk from text file:\n",
      "Content: Python is a high-level, interpreted programming language known for its readability and versatility. Created by Guido van Rossum and released in 1991, it has become one of the most popular languages fo...\n",
      "Metadata: {'source': '../data/text_files/python_intro.txt'}\n",
      "\n",
      "Example chunk from PDF file:\n",
      "Content: NLP,  attention  mechanisms  enable  a  model  to  dynamically  weigh  the  importance  of  different  \n",
      "words\n",
      " \n",
      "in\n",
      " \n",
      "an\n",
      " \n",
      "input\n",
      " \n",
      "sequence,\n",
      " \n",
      "allowing\n",
      " \n",
      "it\n",
      " \n",
      "to\n",
      " \n",
      "focus\n",
      " \n",
      "on\n",
      " \n",
      "the\n",
      " \n",
      "most\n",
      " \n",
      "relevant\n",
      " \n",
      "...\n",
      "Metadata: {'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'attention', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Import text splitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,        # Maximum characters per chunk\n",
    "    chunk_overlap=200,      # Overlap between chunks to maintain context\n",
    "    length_function=len,    # Function to measure chunk length\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Separators to use for splitting\n",
    ")\n",
    "\n",
    "# Combine ALL documents (text + PDF) before chunking\n",
    "print(\"Combining text and PDF documents...\")\n",
    "all_documents = documents + pdf_documents\n",
    "\n",
    "print(f\"Text documents: {len(documents)}\")\n",
    "print(f\"PDF documents: {len(pdf_documents)}\")\n",
    "print(f\"Total documents: {len(all_documents)}\")\n",
    "\n",
    "# Split ALL documents into chunks\n",
    "print(\"\\nSplitting documents into chunks...\")\n",
    "chunks = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"Number of original documents: {len(all_documents)}\")\n",
    "print(f\"Number of chunks after splitting: {len(chunks)}\")\n",
    "\n",
    "# Show examples from both types\n",
    "print(f\"\\nExample chunk from text file:\")\n",
    "text_chunks = [c for c in chunks if c.metadata.get('source', '').endswith('.txt')]\n",
    "if text_chunks:\n",
    "    print(f\"Content: {text_chunks[0].page_content[:200]}...\")\n",
    "    print(f\"Metadata: {text_chunks[0].metadata}\")\n",
    "\n",
    "print(f\"\\nExample chunk from PDF file:\")\n",
    "pdf_chunks = [c for c in chunks if c.metadata.get('source', '').endswith('.pdf')]\n",
    "if pdf_chunks:\n",
    "    print(f\"Content: {pdf_chunks[0].page_content[:200]}...\")\n",
    "    print(f\"Metadata: {pdf_chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9cbd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chromadb\n",
    "import uuid\n",
    "from chromadb.config import Settings\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "950d4805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension; 384\n",
      "Model loaded successfully. Embedding dimension; 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingMangager at 0x16ab5e900>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingMangager:\n",
    "    \"\"\" Handles document embedding generation using sentenceTransformer\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        Args:\n",
    "            model name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self): # Private model\n",
    "        \"\"\" Loads the Sentence transformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension; {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\" Generate embeddings for a list of texts\n",
    "            Args:\n",
    "                texts: List of text strings to embed\n",
    "            Returns:\n",
    "                numpy array of embeddings with shape(len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} text...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generating embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    # initialize the embedding manager\n",
    "embedding_manager = EmbeddingMangager()\n",
    "embedding_manager\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85570845",
   "metadata": {},
   "source": [
    "# Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d48b18f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x16abcc650>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\" Manages documents embeddings in a ChromaDB vector store\"\"\"\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\" Initialize the vector Store\n",
    "            Args: \n",
    "                collection_name: Name of the ChromaDB collection\n",
    "                persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "        \n",
    "    def _initialize_store(self):\n",
    "        \"\"\" Initialize ChromaDB and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok = True)\n",
    "            self.client = chromadb.PersistentClient(path= self.persist_directory)\n",
    "\n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name= self.collection_name,\n",
    "                metadata={\"description\" : \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector Store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing ChromaDB: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\" Add documents and their embeddings to the vector Store\n",
    "        Args: \n",
    "            documents: List of Langchain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store\")\n",
    "\n",
    "        # prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,  # Fixed: was 'metadata', should be 'metadatas'\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "            \n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "939d4422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content=\"Python is a high-level, interpreted programming language known for its readability and versatility. Created by Guido van Rossum and released in 1991, it has become one of the most popular languages for various applications.\\nKey Characteristics:\\nReadability: Python's syntax emphasizes clarity and conciseness, often allowing developers to express concepts in fewer lines of code compared to other languages. This is partly due to its use of indentation to define code blocks.\\nInterpreted: Python code is executed line by line by an interpreter, which facilitates rapid prototyping and interactive testing.\\nDynamically Typed: Variable types are automatically determined at runtime, simplifying code writing as explicit type declarations are not always required.\\nMulti-paradigm: Python supports various programming paradigms, including object-oriented, procedural, and functional programming.\"),\n",
       " Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Multi-paradigm: Python supports various programming paradigms, including object-oriented, procedural, and functional programming.\\nExtensive Standard Library: Python comes with a large standard library that provides modules and functions for a wide range of tasks, reducing the need to write code from scratch.\\nCross-platform: Python applications can be developed and run on different operating systems like Windows, macOS, and Linux.\\nCommon Applications:\\nWeb Development: Used for server-side web applications with frameworks like Django and Flask.\\nData Science and Machine Learning: A popular choice due to its powerful libraries such as NumPy, Pandas, and scikit-learn.\\nAutomation and Scripting: Ideal for automating repetitive tasks and system administration.\\nSoftware Development: Used for creating desktop applications, games, and internal tools.\\nScientific Computing and Research: Employed in various scientific fields for data analysis and modeling.'),\n",
       " Document(metadata={'source': '../data/text_files/machine_learning.txt'}, page_content='Machine learning allows computers to learn from data and improve performance on tasks without explicit programming. The core process involves collecting and preparing data, selecting an algorithm, training a model, and evaluating its accuracy to make predictions. The three primary types of machine learning are supervised learning (using labeled data for tasks like classification and regression), unsupervised learning (finding patterns in unlabeled data, like clustering), and reinforcement learning (learning from rewards and penalties in an environment).\\nKey Concepts\\nData is Crucial: High-quality, diverse data is the foundation of machine learning, providing the examples for models to learn from. \\nAlgorithms & Models: An algorithm is a set of instructions that enables the computer to learn from data, while a model is the trained output of the algorithm. \\nFeatures: These are the attributes or characteristics extracted from data that are used by the model to learn and make decisions.'),\n",
       " Document(metadata={'source': '../data/text_files/machine_learning.txt'}, page_content='Features: These are the attributes or characteristics extracted from data that are used by the model to learn and make decisions. \\nTypes of Machine Learning\\nSupervised Learning:\\nHow it works: Uses labeled datasets, where the correct input-output relationships are known, to train the model. \\nExamples: Image recognition (labeling photos as \"cat\" or \"dog\") and spam email filtering. \\nUnsupervised Learning:\\nHow it works: Works with unlabeled data to identify hidden patterns, similarities, and groupings on its own. \\nExamples: Clustering data points to find common characteristics or detecting anomalies in datasets. \\nReinforcement Learning:\\nHow it works: An agent learns by interacting with an environment, receiving feedback in the form of rewards or penalties for its actions. \\nExamples: Training a robot to navigate or a program to play a game by playing against itself.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'attention', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='NLP,  attention  mechanisms  enable  a  model  to  dynamically  weigh  the  importance  of  different  \\nwords\\n \\nin\\n \\nan\\n \\ninput\\n \\nsequence,\\n \\nallowing\\n \\nit\\n \\nto\\n \\nfocus\\n \\non\\n \\nthe\\n \\nmost\\n \\nrelevant\\n \\nparts\\n \\nof\\n \\nthe\\n \\ncontext\\n \\nwhen\\n \\nprocessing\\n \\ninformation\\n \\nor\\n \\ngenerating\\n \\noutput.\\n \\nThis\\n \\nimproves\\n \\nunderstanding\\n \\nof\\n \\nlong-range\\n \\ndependencies,\\n \\nlike\\n \\n\"dog\"\\n \\nand\\n \\n\"field\"\\n \\nin\\n \\n\"The\\n \\ndog\\n \\nran\\n \\nacross\\n \\nthe\\n \\nfield,\"\\n \\nand\\n \\nis\\n \\na\\n \\nfoundational\\n component  of  modern  Transformer  models,  powering  tasks  from  text  generation to  translation.    How  Attention  Works  At  its  core,  attention  works  by  using  the  concepts  of  queries,  keys,  and  values  to  determine  how  \\nrelevant\\n \\ndifferent\\n \\nparts\\n \\nof\\n \\nthe\\n \\ninput\\n \\nsequence\\n \\nare\\n \\nto\\n \\na\\n \\ngiven\\n \\nword\\n \\nor\\n \\noutput.\\n \\n ●  Query  (Q):  Represents  the  current  input  (e.g.,  the  word  being  processed)  that  is  looking  \\nfor\\n \\nrelevant\\n \\ninformation.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'attention', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='sequence\\n \\nare\\n \\nto\\n \\na\\n \\ngiven\\n \\nword\\n \\nor\\n \\noutput.\\n \\n ●  Query  (Q):  Represents  the  current  input  (e.g.,  the  word  being  processed)  that  is  looking  \\nfor\\n \\nrelevant\\n \\ninformation.\\n \\n ●   ●  Keys  (K):  Act  as  \"labels\"  or  identifiers  for  each  part  of  the  input  data.   ●   ●  Values  (V):  Contain  the  actual  information  that  will  be  retrieved  or  weighted.   ●   The  model  calculates  an  \"attention  score\"  by  comparing  the  query  with  each  key.These  scores  are  then  normalized  using  a  softmax  function to  produce  attention  weights.  Finally,  these  weights  are  used  to  compute  a  weighted  sum  of  the  corresponding  values,  producing  a  refined  \\nrepresentation\\n \\nthat\\n \\nemphasizes\\n \\nthe\\n \\nmost\\n \\nimportant\\n \\ncontext.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'attention', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='representation\\n \\nthat\\n \\nemphasizes\\n \\nthe\\n \\nmost\\n \\nimportant\\n \\ncontext.\\n \\n Key  Types  of  Attention  ●  Self-Attention:  A  crucial  innovation  in  the  Transformer  architecture,  where  queries,  keys,  and  values  are  all  derived  from  the  same  input  sequence.  This  allows  tokens  within  the  \\nsequence\\n \\nto\\n \\nattend\\n \\nto\\n \\neach\\n \\nother,\\n \\ncapturing\\n \\ninternal\\n \\nrelationships\\n \\nand\\n \\ncontext.\\n \\n ●  Multi-Head  Attention:  In  this  technique,  multiple  attention  mechanisms  are  run  in  parallel.  Each  \"head\"  can  learn  different  types  of  relationships,  allowing  the  model  to  grasp  \\nvarious\\n \\npatterns\\n \\nand\\n \\nnuances\\n \\nwithin\\n \\nthe\\n \\ntext.\\n \\n Why  Attention  is  Important  ●  Improved  Contextual  Understanding:  Attention  allows  models  to  capture  dependencies  \\nbetween\\n \\nwords\\n \\nthat\\n \\nare\\n \\nfar\\n \\napart\\n \\nin\\n \\na\\n \\nsentence,\\n \\nleading\\n \\nto\\n \\na\\n \\nricher\\n \\nunderstanding\\n \\nof\\n \\nmeaning.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'attention', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='between\\n \\nwords\\n \\nthat\\n \\nare\\n \\nfar\\n \\napart\\n \\nin\\n \\na\\n \\nsentence,\\n \\nleading\\n \\nto\\n \\na\\n \\nricher\\n \\nunderstanding\\n \\nof\\n \\nmeaning.\\n \\n ●  Enhanced  Performance:  It  has  significantly  improved  performance  in  many  NLP  tasks,  \\nincluding\\n \\ntext\\n \\ngeneration,\\n \\ntranslation,\\n \\nand\\n \\ntext\\n \\nsummarization.\\n \\n ●  Computational  Efficiency:  By  enabling  parallel  processing  of  sequences,  attention  mechanisms  are  more  computationally  efficient  than  older,  recurrent  neural  network  (RNN) based  models,  which  process  information  sequentially.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-08-07T19:40:52+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-08-07T19:40:52+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '../data/pdf_files/Technical_Topics_Cheat_Sheet.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='End-to-End ML Pipeline\\nImagine you’re baking a cake. You gather ingredients (data ingestion), mix and prepare them (data\\ncleaning, feature engineering), bake the cake (train your model), taste-test it (evaluate), then serve it to\\nothers (deploy and monitor). That’s an ML pipeline — taking data from raw to useful predictions.\\nSupervised, Unsupervised & Reinforcement Learning\\nSupervised: You give the computer the question and the answer, and it learns to match them (like\\nteaching with flashcards). Unsupervised: You just give the computer the data, and it finds patterns on\\nits own (like sorting LEGO pieces by shape without labels). Reinforcement: The computer learns by\\ntrying, getting rewards for good actions (like a dog learning tricks with treats).\\nTime Series Forecasting & Monte Carlo Simulation\\nTime series is like trying to guess tomorrow’s weather based on past patterns. Monte Carlo simulation'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-08-07T19:40:52+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-08-07T19:40:52+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '../data/pdf_files/Technical_Topics_Cheat_Sheet.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Time Series Forecasting & Monte Carlo Simulation\\nTime series is like trying to guess tomorrow’s weather based on past patterns. Monte Carlo simulation\\nis like rolling dice many times to guess the chances of something happening — helpful when you’re\\nunsure and want to understand all possible outcomes.\\nNLP & LLMs\\nNLP is how computers read, understand, and write human language. LLMs (like ChatGPT) are trained\\non lots of text and can answer questions, write summaries, or even draft emails — like a very smart\\nintern who reads a million books a day.\\nMLOps + Cloud Deployment\\nMLOps is like DevOps for ML — it helps you build, ship, and maintain ML models smoothly. Cloud tools\\nlike AWS let you run projects on someone else’s supercomputer, so you can scale models reliably, like\\nturning a food truck into a restaurant chain.\\nData Visualization & Storytelling\\nData visualization turns numbers into pictures so people can see patterns — think graphs, charts,'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-08-07T19:40:52+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-08-07T19:40:52+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '../data/pdf_files/Technical_Topics_Cheat_Sheet.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content=\"turning a food truck into a restaurant chain.\\nData Visualization & Storytelling\\nData visualization turns numbers into pictures so people can see patterns — think graphs, charts,\\ndashboards. Storytelling is explaining those patterns so someone can make a smart decision — like\\nturning a traffic map into 'Take Route B — it’s faster!'\\nReal-World Data (RWD)\\nRWD is messy, like kids’ drawings on paper — lots of creativity, but not always clean. It comes from\\nreal people: hospital records, prescriptions, sales logs. You have to clean it, match things up, and make\\nsure it’s trustworthy before using it.\\nVersion Control & Collaboration (Git, Agile)\\nGit is like Google Docs for code — it tracks changes, lets you go back, and lets your team work\\ntogether. Agile means breaking big jobs into small pieces, sharing updates often, and adjusting as you\\ngo — like building LEGO castles one section at a time.\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'embedding', 'source': '../data/pdf_files/embedding.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='●   An  embedding  is  a  data  representation  that  uses  low-dimensional  numerical  vectors  to  capture  \\nthe\\n \\nsemantic\\n \\nmeaning\\n \\nand\\n \\nrelationships\\n \\nof\\n \\nnon-numerical\\n \\ndata\\n \\nlike\\n \\nwords,\\n \\nimages,\\n \\nor\\n \\ngraphs,\\n \\nmaking\\n \\nthem\\n \\nunderstandable\\n \\nfor\\n \\nmachine\\n \\nlearning\\n \\nmodels.\\n \\nThese\\n \\ndense\\n \\nvectors\\n \\nare\\n \\ncreated\\n \\nthrough\\n \\na\\n \\nprocess\\n \\nof\\n \\nmapping\\n \\ncomplex,\\n \\nreal-world\\n \\ndata\\n \\ninto\\n \\na\\n \\nvector\\n \\nspace,\\n \\nwhere\\n \\nthe\\n \\ndistance\\n \\nbetween\\n \\nembeddings\\n \\nreflects\\n \\nthe\\n \\nsimilarity\\n \\nof\\n \\nthe\\n \\noriginal\\n \\ndata\\n \\npoints.\\n \\nFor\\n \\ninstance,\\n \\nin\\n \\nnatural\\n \\nlanguage\\n \\nprocessing\\n \\n(NLP),\\n \\nword\\n \\nembeddings\\n \\nallow\\n \\nfor\\n \\nmathematical\\n \\noperations\\n \\nthat\\n \\nrepresent\\n \\nrelationships,\\n \\nlike\\n \\nking\\n \\n-\\n \\nman\\n \\n+\\n \\nwoman\\n \\n≈\\n \\nqueen,\\n \\nto\\n \\nuncover\\n \\ndeeper\\n \\nmeaning\\n \\nand\\n \\nenable\\n \\ntasks\\n \\nsuch\\n \\nas\\n \\nsentiment\\n \\nanalysis\\n \\nand\\n \\nmachine\\n \\ntranslation.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'embedding', 'source': '../data/pdf_files/embedding.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='relationships,\\n \\nlike\\n \\nking\\n \\n-\\n \\nman\\n \\n+\\n \\nwoman\\n \\n≈\\n \\nqueen,\\n \\nto\\n \\nuncover\\n \\ndeeper\\n \\nmeaning\\n \\nand\\n \\nenable\\n \\ntasks\\n \\nsuch\\n \\nas\\n \\nsentiment\\n \\nanalysis\\n \\nand\\n \\nmachine\\n \\ntranslation.\\n  \\n What  is  the  process?  ●  Data  Transformation:  Non-numerical  data  (text,  images,  graphs)  is  converted  into  numerical  vectors,  where  each  dimension  represents  a  specific  feature  of  the  data.   ●  Vector  Space:  These  vectors  are  placed  into  an  n-dimensional  space,  creating  a  dense  numerical  representation.   ●  Semantic  Relationships:  The  embedding  process  captures  the  nuances  and  context  of  the  original  data.  For  example,  words  with  similar  meanings  will  have  embeddings  that  \\nare\\n \\ncloser\\n \\ntogether\\n \\nin\\n \\nthe\\n \\nvector\\n \\nspace.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'embedding', 'source': '../data/pdf_files/embedding.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='are\\n \\ncloser\\n \\ntogether\\n \\nin\\n \\nthe\\n \\nvector\\n \\nspace.\\n \\n ●  Machine  Learning  Use:  These  vectors  can  then  be  used  by  machine  learning  models  for  tasks  like  classification,  search,  recommendation  systems,  and  more.   Key  Characteristics  and  Benefits  ●  Information  Density:  Embeddings  provide  an  information-dense  representation,  meaning  \\nthey\\n \\npack\\n \\na\\n \\nlot\\n \\nof\\n \\nmeaning\\n \\ninto\\n \\nfewer\\n \\ndimensions\\n \\nthan\\n \\ntraditional\\n \\nmethods.\\n \\n ●  Semantic  Similarity:  The  distance  between  two  embeddings  in  the  vector  space  \\ncorrelates\\n \\nwith\\n \\nthe\\n \\nsemantic\\n \\nsimilarity\\n \\nof\\n \\nthe\\n \\noriginal\\n \\ninputs.\\n \\n ●  Machine-Readable  Format:  Embeddings  make  complex  data  understandable  to  \\ncomputers\\n \\nand\\n \\nalgorithms,\\n \\nwhich\\n \\nonly\\n \\nprocess\\n \\nnumerical\\n \\ninputs.\\n \\n ●  Capture  of  Relationships:  Embeddings  can  capture  intricate  relationships  within  the  data,  \\nsuch\\n \\nas\\n \\nco-occurrence\\n \\npatterns\\n \\nin\\n \\ntext\\n \\nor\\n \\nstructural\\n \\nrelationships'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'embedding', 'source': '../data/pdf_files/embedding.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='●  Capture  of  Relationships:  Embeddings  can  capture  intricate  relationships  within  the  data,  \\nsuch\\n \\nas\\n \\nco-occurrence\\n \\npatterns\\n \\nin\\n \\ntext\\n \\nor\\n \\nstructural\\n \\nrelationships\\n \\nin\\n \\ngraphs.\\n \\n Examples  of  Embeddings  ●  Word  Embeddings:  Represent  words  as  vectors,  enabling  tasks  like  sentiment  analysis  and  machine  translation.   ●  Image  Embeddings:  Map  images  into  a  vector  space  to  group  similar  images  and  enable  visual  content  retrieval.   ●  Graph  Embeddings:  Convert  nodes  and  edges  in  graphs  into  numerical  vectors  to  understand  relationships  and  predict  links.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba4789e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 15 text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with shape: (15, 384)\n",
      "Adding 15 documents to vector store\n",
      "Successfully added 15 documents to vector store\n",
      "Total documents in collection: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the text to embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "# Generate the Embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# Store in the vector Database\n",
    "vectorstore.add_documents(chunks, embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339592a",
   "metadata": {},
   "source": [
    "# Retriver Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96e94e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriver at 0x17b214f80>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RAGRetriver:\n",
    "    \"\"\" Handles query based retrieval from the vector store\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingMangager):\n",
    "        \"\"\" Initialize the retriver\n",
    "        Args:\n",
    "            vectorestore: Vector Store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager \n",
    "        \n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\" Retrieve relevant documents for a query\n",
    "            Args: \n",
    "            query: The search query\n",
    "            top_k: number of top results to return \n",
    "            score_threshold: Minimum similarity score threshold\n",
    "\n",
    "            Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "\n",
    "        # generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            retrieved_docs = []\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            return retrieved_docs\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval:{e}\")\n",
    "            return []\n",
    "        \n",
    "rag_retriever = RAGRetriver(vectorstore, embedding_manager)\n",
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "249ca7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'why attention is important'\n",
      "top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_afd086a1_6',\n",
       "  'content': 'representation\\n \\nthat\\n \\nemphasizes\\n \\nthe\\n \\nmost\\n \\nimportant\\n \\ncontext.\\n \\n Key  Types  of  Attention  ●  Self-Attention:  A  crucial  innovation  in  the  Transformer  architecture,  where  queries,  keys,  and  values  are  all  derived  from  the  same  input  sequence.  This  allows  tokens  within  the  \\nsequence\\n \\nto\\n \\nattend\\n \\nto\\n \\neach\\n \\nother,\\n \\ncapturing\\n \\ninternal\\n \\nrelationships\\n \\nand\\n \\ncontext.\\n \\n ●  Multi-Head  Attention:  In  this  technique,  multiple  attention  mechanisms  are  run  in  parallel.  Each  \"head\"  can  learn  different  types  of  relationships,  allowing  the  model  to  grasp  \\nvarious\\n \\npatterns\\n \\nand\\n \\nnuances\\n \\nwithin\\n \\nthe\\n \\ntext.\\n \\n Why  Attention  is  Important  ●  Improved  Contextual  Understanding:  Attention  allows  models  to  capture  dependencies  \\nbetween\\n \\nwords\\n \\nthat\\n \\nare\\n \\nfar\\n \\napart\\n \\nin\\n \\na\\n \\nsentence,\\n \\nleading\\n \\nto\\n \\na\\n \\nricher\\n \\nunderstanding\\n \\nof\\n \\nmeaning.',\n",
       "  'metadata': {'title': 'attention',\n",
       "   'page_label': '1',\n",
       "   'producer': 'Skia/PDF m142 Google Docs Renderer',\n",
       "   'content_length': 928,\n",
       "   'creationdate': '',\n",
       "   'page': 0,\n",
       "   'creator': 'PyPDF',\n",
       "   'total_pages': 1,\n",
       "   'doc_index': 6,\n",
       "   'source': '../data/pdf_files/attention.pdf'},\n",
       "  'similarity_score': 0.2655085325241089,\n",
       "  'distance': 0.7344914674758911,\n",
       "  'rank': 1}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"why attention is important\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c43a5a",
   "metadata": {},
   "source": [
    "# Generation Pipeline\n",
    "## Integration VectorDB Context Pipeline with LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4b44a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GROQ_API_KEY loaded successfully\n",
      "LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Simple RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from environment variables\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not groq_api_key:\n",
    "    print(\"Warning: GROQ_API_KEY not found in environment variables\")\n",
    "    print(\"Please create a .env file with your GROQ_API_KEY\")\n",
    "else:\n",
    "    print(\"✅ GROQ_API_KEY loaded successfully\")\n",
    "\n",
    "# Initialize the Groq LLM with a currently supported model\n",
    "llm = ChatGroq(\n",
    "    api_key=groq_api_key,  \n",
    "    model=\"llama-3.1-8b-instant\",  # Updated to a currently supported model\n",
    "    temperature=0.1, \n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "print(\"LLM initialized successfully!\")\n",
    "\n",
    "# Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query, retriever, llm, top_k=3):\n",
    "    \"\"\"\n",
    "    Simple RAG pipeline that retrieves context and generates an answer\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Processing query: '{query}'\")\n",
    "    \n",
    "    # Retrieve the context\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "    \n",
    "    if not results:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    # Combine retrieved documents into context\n",
    "    context_pieces = []\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        context_pieces.append(f\"Context {i}:\\n{doc['content']}\")\n",
    "        print(f\"✅ Retrieved context {i} (score: {doc['similarity_score']:.3f})\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_pieces)\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = f\"\"\"Use the following context to answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    print(\"🤖 Generating response...\")\n",
    "    \n",
    "    # Generate the answer using Groq LLM\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "851e1746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: llama-3.1-8b-instant\n",
      "✅ llama-3.1-8b-instant works!\n",
      "\n",
      "🎉 Using working model: llama-3.1-8b-instant\n",
      "✅ llama-3.1-8b-instant works!\n",
      "\n",
      "🎉 Using working model: llama-3.1-8b-instant\n"
     ]
    }
   ],
   "source": [
    "# Test different Groq models to find one that works\n",
    "def test_groq_models():\n",
    "    \"\"\"Test various Groq models to find supported ones\"\"\"\n",
    "    test_models = [\n",
    "        \"llama-3.1-8b-instant\",\n",
    "        \"llama-3.2-90b-text-preview\", \n",
    "        \"llama-3.2-11b-text-preview\",\n",
    "        \"llama-3.2-3b-preview\",\n",
    "        \"llama-3.2-1b-preview\",\n",
    "        \"mixtral-8x7b-32768\",\n",
    "        \"gemma2-9b-it\"\n",
    "    ]\n",
    "    \n",
    "    groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    if not groq_api_key:\n",
    "        print(\"No API key found\")\n",
    "        return None\n",
    "    \n",
    "    for model in test_models:\n",
    "        try:\n",
    "            print(f\"Testing model: {model}\")\n",
    "            test_llm = ChatGroq(api_key=groq_api_key, model=model, temperature=0.1, max_tokens=50)\n",
    "            response = test_llm.invoke(\"Hello, this is a test.\")\n",
    "            print(f\"✅ {model} works!\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {model} failed: {str(e)[:100]}...\")\n",
    "            continue\n",
    "    \n",
    "    print(\"No working models found\")\n",
    "    return None\n",
    "\n",
    "# Find a working model\n",
    "working_model = test_groq_models()\n",
    "if working_model:\n",
    "    print(f\"\\n🎉 Using working model: {working_model}\")\n",
    "    llm = ChatGroq(api_key=groq_api_key, model=working_model, temperature=0.1, max_tokens=1024)\n",
    "else:\n",
    "    print(\"❌ No supported models found. Please check Groq documentation for current models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3849161f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Processing query: 'what is attention mechanism?'\n",
      "Retrieving documents for query: 'what is attention mechanism?'\n",
      "top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n",
      "✅ Retrieved context 1 (score: 0.171)\n",
      "✅ Retrieved context 2 (score: 0.148)\n",
      "🤖 Generating response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An attention mechanism is a technique in Natural Language Processing (NLP) that enables a model to dynamically weigh the importance of different words in an input sequence, allowing it to focus on the most relevant parts of the context when processing information or generating output.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"what is attention mechanism?\", rag_retriever, llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71661f36",
   "metadata": {},
   "source": [
    "# Inhance RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b3e110f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is the process in embedding?'\n",
      "top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: The process in embedding involves three main steps:\n",
      "\n",
      "1. **Data Transformation**: Non-numerical data (text, images, graphs) is converted into numerical vectors, where each dimension represents a specific feature of the data.\n",
      "2. **Vector Space**: These vectors are placed into an n-dimensional space, creating a dense numerical representation.\n",
      "3. **Semantic Relationships**: The embedding process captures the nuances and context of the original data, where words with similar meanings will have embeddings that are closer together in the vector space.\n",
      "sources: [{'source': '../data/pdf_files/embedding.pdf', 'page': 0, 'score': 0.36253780126571655, 'preview': '●   An  embedding  is  a  data  representation  that  uses  low-dimensional  numerical  vectors  to  capture  \\nthe\\n \\nsemantic\\n \\nmeaning\\n \\nand\\n \\nrelationships\\n \\nof\\n \\nnon-numerical\\n \\ndata\\n \\nlike\\n \\nwords,\\n \\nimages,\\n \\nor\\n \\ngraphs,\\n \\nmaking\\n \\nthem\\n \\nunderstandable\\n \\nfor\\n \\nmachine\\n \\nlearning\\n \\nmodels.\\n \\nT...'}, {'source': '../data/pdf_files/embedding.pdf', 'page': 0, 'score': 0.12220293283462524, 'preview': '●  Capture  of  Relationships:  Embeddings  can  capture  intricate  relationships  within  the  data,  \\nsuch\\n \\nas\\n \\nco-occurrence\\n \\npatterns\\n \\nin\\n \\ntext\\n \\nor\\n \\nstructural\\n \\nrelationships\\n \\nin\\n \\ngraphs.\\n \\n Examples  of  Embeddings  ●  Word  Embeddings:  Represent  words  as  vectors,  enabling  task...'}, {'source': '../data/pdf_files/embedding.pdf', 'page': 0, 'score': 0.11805272102355957, 'preview': 'relationships,\\n \\nlike\\n \\nking\\n \\n-\\n \\nman\\n \\n+\\n \\nwoman\\n \\n≈\\n \\nqueen,\\n \\nto\\n \\nuncover\\n \\ndeeper\\n \\nmeaning\\n \\nand\\n \\nenable\\n \\ntasks\\n \\nsuch\\n \\nas\\n \\nsentiment\\n \\nanalysis\\n \\nand\\n \\nmachine\\n \\ntranslation.\\n  \\n What  is  the  process?  ●  Data  Transformation:  Non-numerical  data  (text,  images,  graphs)  is  convert...'}]\n",
      "confidence: 0.36253780126571655\n",
      "context preview: ●   An  embedding  is  a  data  representation  that  uses  low-dimensional  numerical  vectors  to  capture  \n",
      "the\n",
      " \n",
      "semantic\n",
      " \n",
      "meaning\n",
      " \n",
      "and\n",
      " \n",
      "relationships\n",
      " \n",
      "of\n",
      " \n",
      "non-numerical\n",
      " \n",
      "data\n",
      " \n",
      "like\n",
      " \n",
      "words,\n",
      " \n",
      "images,\n",
      " \n",
      "or\n",
      " \n",
      "graphs,\n",
      " \n",
      "making\n",
      " \n",
      "them\n",
      " \n",
      "understandable\n",
      " \n",
      "for\n",
      " \n",
      "machine\n",
      " \n",
      "learning\n",
      " \n",
      "models.\n",
      " \n",
      "T\n"
     ]
    }
   ],
   "source": [
    "# Advanced RAG Pipeline features\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\" \n",
    "    RAG Pipeline with extra features:\n",
    "    - Returns answer, source, confidence score, and optionally full context\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold= min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'source':[], 'confidence':0.0, 'context':''}\n",
    "\n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\". join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview':doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "\n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question accurately and concisely.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Answer:\"\"\"   \n",
    "    \n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "\n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "\n",
    "# Example usage\n",
    "result = rag_advanced(\"what is the process in embedding?\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print('answer:', result['answer'])\n",
    "print('sources:', result['sources'])\n",
    "print('confidence:', result['confidence'])\n",
    "print('context preview:', result['context'][:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f544ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "building-rag-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
